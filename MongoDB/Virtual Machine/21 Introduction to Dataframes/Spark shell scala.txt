$ vagrant --version

$ cd "E:\Working\MongoDB\MongoDB University\2016\M233 Getting Started with Spark and MongoDB\Units\Spark and MongoDB\1 Welcome, prerequisites, and setting up your learning environment\Vagrantfile"

$ vagrant up

$ vagrant ssh

$ scala -version

$ ls

# Spark Home
$ ls spark-1.6.1/

# Spark Binaries
$ ls spark-1.6.1/bin/

# Spark Connector Configuration # Command-line # --conf parameter
$ ./spark-1.6.1/bin/spark-shell --packages org.mongodb.spark:mongo-spark-connector_2.10:0.1 --conf "spark.mongodb.input.uri=mongodb://127.0.0.1/nasa.eva" --conf "spark.mongodb.output.uri=mongodb://127.0.0.1/nasa.astronautTotals"

[WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)]
[scala> :help]
[scala> :cp <path>                 add a jar or directory to the classpath]

scala> import com.mongodb.spark._

[scala> import com.mongodb.spark.config._]

scala> import com.mongodb.spark.rdd.MongoRDD


scala> import org.bson.Document

scala> val rdd = sc.loadFromMongoDB()

scala> println( rdd.count() )

scala> println( rdd.first() )

scala> for( doc <- rdd.take ( 10 ) ) println( doc )

[
# Spark Connector Configuration # File in Spark Home Directory # spark-defaults.conf.template
$ ls ./spark-1.6.1/conf
spark-defaults.conf.template

# Spark Connector Configuration # Spark shell # Spark conf object
$SPARK_HOME/conf/spark-default.conf file
]

# ReadConfig
scala> import com.mongodb.spark.config._

scala> val readConf = ReadConfig( sc )

scala> 
val readConfig = ReadConfig( Map("collection" -> "eva" ), Some(readConf))

scala> val newRDD = sc.loadFromMongoDB( readConfig = readConfig )

# Transformation of RDD of Documents into RDD of Tuples
scala> println( rdd.first() )

scala> def  breakoutCrew (  document: Document  ): List[(String,Int)]  = {

  var minutes = 0;
  val timeString = document.get( "Duration").asInstanceOf[String]
  if( timeString != null && !timeString.isEmpty ) {
    val time =  document.get( "Duration").asInstanceOf[String].split( ":" )
    minutes = time(0).toInt * 60 + time(1).toInt
  }

  import scala.util.matching.Regex
  val pattern = new Regex("(\\w+\\s\\w+)")
  val names =  pattern findAllIn document.get( "Crew" ).asInstanceOf[String]
  var tuples : List[(String,Int)] = List()
  for ( name <- names ) { tuples = tuples :+ (( name, minutes ) ) }

  return tuples
}

# MapReduce
scala> val logs = rdd.flatMap( breakoutCrew ).reduceByKey( (m1: Int, m2: Int) => ( m1 + m2 ) )

scala> logs.foreach( println )

# Write RDDs to MongoDB [Document or BSON Document or DBObject]
scala> val writeConf = WriteConfig(sc)

scala> val writeConfig = WriteConfig(Map("collection" -> "astronautTotals", "writeConcern.w" -> "majority", "db" -> "nasa"), Some(writeConf))

scala> def mapToDocument( tuple: (String, Int )  ): Document = {
  val doc = new Document();
  doc.put( "name", tuple._1 )
  doc.put( "minutes", tuple._2 )

  return doc
}

scala> logs.map( mapToDocument ).saveToMongoDB( writeConfig )

# Aggregation pipeline
scala> val rdd = sc.loadFromMongoDB()

scala> val aggRdd = rdd.withPipeline(Seq(Document.parse("{ $match: { Country: \"USA\" } }")))

# Dataframes 
[RDD + Schema = Dataframes]
[Relational Table Equivalent, Distributed Data Structure with Named Columns to execute SQL queries, Syntax based on HIVEQL]
[Dataframes generated from RDDs have their Schema set Programmatically or Through Reflection]
[Schema set Programmatically if the schema is not known until runtime]
[Schema set Through Reflection requires that the schema is known ahead of time, and is less verbose]
scala> import org.apache.spark.sql.SQLContext

scala> import com.mongodb.spark.sql._

# load the first dataframe "EVAs"
scala> val evadf = sqlContext.read.mongo()

[scala> evadf.printSchema()]

scala> evadf.registerTempTable("evas")