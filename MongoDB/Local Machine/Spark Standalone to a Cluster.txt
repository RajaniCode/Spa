# Master
> e:
> cd "E:\Working\MongoDB\Spark\spark-1.6.1\bin"
# OR
> cd "E:\Working\MongoDB\Spark\spark-1.6.1-bin-hadoop2.6\bin"
> spark-class org.apache.spark.deploy.master.Master

http://localhost:8080/
Spark Master at spark://192.168.56.1:7077


# Worker 
> e:
> cd "E:\Working\MongoDB\Spark\spark-1.6.1\bin"
# OR
> cd "E:\Working\MongoDB\Spark\spark-1.6.1-bin-hadoop2.6\bin"
> spark-class org.apache.spark.deploy.worker.Worker spark://192.168.56.1:7077

http://localhost:8080/
Spark Master at spark://192.168.56.1:7077
Workers
Worker Id				Address			State		Cores	 	Memory
worker20170327201831192.168.56.152052
(http://192.168.56.1:8081)		192.168.56.1:50076	ALIVE		4 (0 Used)	2.8 GB (0.0 B Used)


# Spark Shell Scala
> e:
> cd "E:\Working\MongoDB\Spark\spark-1.6.1\bin"
# OR
> cd "E:\Working\MongoDB\Spark\spark-1.6.1-bin-hadoop2.6\bin"
# Environment Variables User Variable HADOOP_HOME
# Note E:\tmp\hive\Aspire
> mkdir "E:\tmp\hive" 
> winutils.exe chmod 777 /tmp/hive
[> spark-shell --master spark://192.168.56.1:7077]
> spark-shell --packages org.mongodb.spark:mongo-spark-connector_2.10:0.1 --conf "spark.mongodb.input.uri=mongodb://127.0.0.1/nasa.eva" --conf "spark.mongodb.output.uri=mongodb://127.0.0.1/nasa.astronautTotals"

# OR

# Java Task
> e:
> cd "E:\Working\MongoDB\Spark\spark-1.6.1\bin"
# OR
> cd "E:\Working\MongoDB\Spark\spark-1.6.1-bin-hadoop2.6\bin"
> spark-submit --class spacewalk.SpaceWalk --master spark://192.168.56.1:7077 "E:\Working\MongoDB\Spark\MongoDB_Spark_Course\build\libs\SparkCourse-1.0.0-SNAPSHOT.jar"