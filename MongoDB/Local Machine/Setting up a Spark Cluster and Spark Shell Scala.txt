# Master
> e:
> cd "E:\Working\MongoDB\Spark\spark-1.6.1\bin"
# OR
> cd "E:\Working\MongoDB\Spark\spark-1.6.1-bin-hadoop2.6\bin"
> spark-class org.apache.spark.deploy.master.Master

http://localhost:8080/
Spark Master at spark://192.168.56.1:7077


# Worker 
> e:
> cd "E:\Working\MongoDB\Spark\spark-1.6.1\bin"
# OR
> cd "E:\Working\MongoDB\Spark\spark-1.6.1-bin-hadoop2.6\bin"
> spark-class org.apache.spark.deploy.worker.Worker spark://192.168.56.1:7077

http://localhost:8080/
Spark Master at spark://192.168.56.1:7077
Workers
Worker Id				Address			State		Cores	 	Memory
worker20170327201831192.168.56.152052
(http://192.168.56.1:8081)		192.168.56.1:50076	ALIVE		4 (0 Used)	2.8 GB (0.0 B Used)


# Spark Shell Scala
> e:
> cd "E:\Working\MongoDB\Spark\spark-1.6.1\bin"
# OR
> cd "E:\Working\MongoDB\Spark\spark-1.6.1-bin-hadoop2.6\bin"
# Environment Variables User Variable HADOOP_HOME
# Note E:\tmp\hive\Aspire
> mkdir "E:\tmp\hive" 
> winutils.exe chmod 777 /tmp/hive
[> spark-shell --master spark://192.168.56.1:7077]
> spark-shell --packages org.mongodb.spark:mongo-spark-connector_2.10:0.1 --conf "spark.mongodb.input.uri=mongodb://127.0.0.1/nasa.eva" --conf "spark.mongodb.output.uri=mongodb://127.0.0.1/nasa.astronautTotals"

# Spark Connector Configuration # Command-line # --conf parameter
> spark-shell --packages org.mongodb.spark:mongo-spark-connector_2.10:0.1 --conf "spark.mongodb.input.uri=mongodb://127.0.0.1/nasa.eva" --conf "spark.mongodb.output.uri=mongodb://127.0.0.1/nasa.astronautTotals"

scala> import com.mongodb.spark._

[scala> import com.mongodb.spark.config._]

scala> import com.mongodb.spark.rdd.MongoRDD


scala> import org.bson.Document

scala> val rdd = sc.loadFromMongoDB()

scala> println( rdd.count() )

scala> println( rdd.first() )

scala> for( doc <- rdd.take ( 10 ) ) println( doc )

[
# Spark Connector Configuration # File in Spark Home Directory # spark-defaults.conf.template
$ ls ./spark-1.6.1/conf
spark-defaults.conf.template

# Spark Connector Configuration # Spark shell # Spark conf object
$SPARK_HOME/conf/spark-default.conf file
]

# ReadConfig
scala> import com.mongodb.spark.config._

scala> val readConf = ReadConfig( sc )

scala> 
val readConfig = ReadConfig( Map("collection" -> "eva" ), Some(readConf))

scala> val newRDD = sc.loadFromMongoDB( readConfig = readConfig )

# Transformation of RDD of Documents into RDD of Tuples
scala> println( rdd.first() )

scala> def  breakoutCrew (  document: Document  ): List[(String,Int)]  = {

  var minutes = 0;
  val timeString = document.get( "Duration").asInstanceOf[String]
  if( timeString != null && !timeString.isEmpty ) {
    val time =  document.get( "Duration").asInstanceOf[String].split( ":" )
    minutes = time(0).toInt * 60 + time(1).toInt
  }

  import scala.util.matching.Regex
  val pattern = new Regex("(\\w+\\s\\w+)")
  val names =  pattern findAllIn document.get( "Crew" ).asInstanceOf[String]
  var tuples : List[(String,Int)] = List()
  for ( name <- names ) { tuples = tuples :+ (( name, minutes ) ) }

  return tuples
}

# MapReduce
scala> val logs = rdd.flatMap( breakoutCrew ).reduceByKey( (m1: Int, m2: Int) => ( m1 + m2 ) )

scala> logs.foreach( println )

# Write RDDs to MongoDB [Document or BSON Document or DBObject]
scala> val writeConf = WriteConfig(sc)

scala> val writeConfig = WriteConfig(Map("collection" -> "astronautTotals", "writeConcern.w" -> "majority", "db" -> "nasa"), Some(writeConf))

scala> def mapToDocument( tuple: (String, Int )  ): Document = {
  val doc = new Document();
  doc.put( "name", tuple._1 )
  doc.put( "minutes", tuple._2 )

  return doc
}

scala> logs.map( mapToDocument ).saveToMongoDB( writeConfig )

# Aggregation pipeline
scala> val rdd = sc.loadFromMongoDB()

scala> val aggRdd = rdd.withPipeline(Seq(Document.parse("{ $match: { Country: \"USA\" } }")))

# Dataframes 
[RDD + Schema = Dataframes]
[Relational Table Equivalent, Distributed Data Structure with Named Columns to execute SQL queries, Syntax based on HIVEQL]
[Dataframes generated from RDDs have their Schema set Programmatically or Through Reflection]
[Schema set Programmatically if the schema is not known until runtime]
[Schema set Through Reflection requires that the schema is known ahead of time, and is less verbose]
scala> import org.apache.spark.sql.SQLContext

scala> import com.mongodb.spark.sql._

# load the first dataframe "EVAs"
scala> val evadf = sqlContext.read.mongo()

[scala> evadf.printSchema()]

scala> evadf.registerTempTable("evas")

# Interactions with dataframes 
[Dataframe is equivalent to distributed in-memory Table]
[To generate Dataframe, assign Schema to RDD i.e., RDD + Schema = Dataframes]
# Show first 20 records
scala> evadf.show()

scala> evadf.count()

# Examine Schema
scala> evadf.printSchema()

# Case class to define Schema Through Reflection
scala> case class astronautTotal ( name: String, minutes: Integer )

scala> val astronautDF = sqlContext.read.option("collection", "astronautTotals").mongo[astronautTotal]()

[scala> astronautDF.printSchema()]



# To issue SQL queries against a dataframe we must first register it as a temptable
scala> astronautDF.registerTempTable("astronautTotals")

scala> astronautDF.printSchema()

# Select
scala> sqlContext.sql("SELECT astronautTotals.name, astronautTotals.minutes FROM astronautTotals"  ).show()


# Join


scala> sqlContext.sql("SELECT astronautTotals.name, astronautTotals.minutes, evas.Vehicle, evas.Duration FROM " +
  "astronautTotals JOIN evas ON astronautTotals.name LIKE evas.Crew"  ).show()

# After Writing the RDDs to MongoDB
[
$ db.astronautTotals.find().pretty()
]