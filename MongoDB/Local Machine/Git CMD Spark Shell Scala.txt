C:\Users\Aspire>java -version
java version "1.8.0_102"
Java(TM) SE Runtime Environment (build 1.8.0_102-b14)
Java HotSpot(TM) 64-Bit Server VM (build 25.102-b14, mixed mode)

C:\Users\Aspire>mongod --version
db version v3.2.8
git version: ed70e33130c977bda0024c125b56d159573dbaf0
OpenSSL version: OpenSSL 1.0.1p-fips 9 Jul 2015
allocator: tcmalloc
modules: none
build environment:
    distmod: 2008plus-ssl
    distarch: x86_64
    target_arch: x86_64

C:\Users\Aspire>e:

E:\>cd E:\Working\MongoDB\Spark\spark-1.6.1\bin

E:\Working\MongoDB\Spark\spark-1.6.1\bin>mkdir "E:\tmp\hive"

E:\Working\MongoDB\Spark\spark-1.6.1\bin>winutils.exe chmod 777 /tmp/hive

E:\Working\MongoDB\Spark\spark-1.6.1\bin>spark-shell --packages org.mongodb.spark:mongo-spark-connector_2.10:0.1 --conf "spark.mongodb.input.uri=mongodb://127.0.0.1/nasa.eva" --con
f "spark.mongodb.output.uri=mongodb://127.0.0.1/nasa.astronautTotals"
Ivy Default Cache set to: C:\Users\Aspire\.ivy2\cache
The jars for the packages stored in: C:\Users\Aspire\.ivy2\jars
:: loading settings :: url = jar:file:/E:/Working/MongoDB/Spark/spark-1.6.1/lib/spark-assembly-1.6.1-hadoop2.6.0.jar!/org/apache/ivy/core/settings/ivysettings.xml
org.mongodb.spark#mongo-spark-connector_2.10 added as a dependency
:: resolving dependencies :: org.apache.spark#spark-submit-parent;1.0
        confs: [default]
        found org.mongodb.spark#mongo-spark-connector_2.10;0.1 in central
        found org.mongodb#mongo-java-driver;3.2.2 in central
:: resolution report :: resolve 655ms :: artifacts dl 14ms
        :: modules in use:
        org.mongodb#mongo-java-driver;3.2.2 from central in [default]
        org.mongodb.spark#mongo-spark-connector_2.10;0.1 from central in [default]
        ---------------------------------------------------------------------
        |                  |            modules            ||   artifacts   |
        |       conf       | number| search|dwnlded|evicted|| number|dwnlded|
        ---------------------------------------------------------------------
        |      default     |   2   |   0   |   0   |   0   ||   2   |   0   |
        ---------------------------------------------------------------------
:: retrieving :: org.apache.spark#spark-submit-parent
        confs: [default]
        0 artifacts copied, 2 already retrieved (0kB/22ms)
log4j:WARN No appenders could be found for logger (org.apache.hadoop.metrics2.lib.MutableMetricsFactory).
log4j:WARN Please initialize the log4j system properly.
log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
Using Spark's repl log4j profile: org/apache/spark/log4j-defaults-repl.properties
To adjust logging level use sc.setLogLevel("INFO")
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.1
      /_/

Using Scala version 2.10.5 (Java HotSpot(TM) 64-Bit Server VM, Java 1.8.0_102)
Type in expressions to have them evaluated.
Type :help for more information.
Spark context available as sc.
17/03/27 20:43:10 WARN General: Plugin (Bundle) "org.datanucleus.api.jdo" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The
 URL "file:/E:/Working/MongoDB/Spark/spark-1.6.1/lib/datanucleus-api-jdo-3.2.6.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/
E:/Working/MongoDB/Spark/spark-1.6.1/bin/../lib/datanucleus-api-jdo-3.2.6.jar."
17/03/27 20:43:10 WARN General: Plugin (Bundle) "org.datanucleus.store.rdbms" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath.
 The URL "file:/E:/Working/MongoDB/Spark/spark-1.6.1/lib/datanucleus-rdbms-3.2.9.jar" is already registered, and you are trying to register an identical plugin located at URL "file
:/E:/Working/MongoDB/Spark/spark-1.6.1/bin/../lib/datanucleus-rdbms-3.2.9.jar."
17/03/27 20:43:10 WARN General: Plugin (Bundle) "org.datanucleus" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL "fi
le:/E:/Working/MongoDB/Spark/spark-1.6.1/bin/../lib/datanucleus-core-3.2.10.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/E:/
Working/MongoDB/Spark/spark-1.6.1/lib/datanucleus-core-3.2.10.jar."
17/03/27 20:43:10 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/27 20:43:11 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/27 20:43:30 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/03/27 20:43:30 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/03/27 20:43:40 WARN General: Plugin (Bundle) "org.datanucleus.api.jdo" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The
 URL "file:/E:/Working/MongoDB/Spark/spark-1.6.1/lib/datanucleus-api-jdo-3.2.6.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/
E:/Working/MongoDB/Spark/spark-1.6.1/bin/../lib/datanucleus-api-jdo-3.2.6.jar."
17/03/27 20:43:40 WARN General: Plugin (Bundle) "org.datanucleus.store.rdbms" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath.
 The URL "file:/E:/Working/MongoDB/Spark/spark-1.6.1/lib/datanucleus-rdbms-3.2.9.jar" is already registered, and you are trying to register an identical plugin located at URL "file
:/E:/Working/MongoDB/Spark/spark-1.6.1/bin/../lib/datanucleus-rdbms-3.2.9.jar."
17/03/27 20:43:40 WARN General: Plugin (Bundle) "org.datanucleus" is already registered. Ensure you dont have multiple JAR versions of the same plugin in the classpath. The URL "fi
le:/E:/Working/MongoDB/Spark/spark-1.6.1/bin/../lib/datanucleus-core-3.2.10.jar" is already registered, and you are trying to register an identical plugin located at URL "file:/E:/
Working/MongoDB/Spark/spark-1.6.1/lib/datanucleus-core-3.2.10.jar."
17/03/27 20:43:40 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/03/27 20:43:41 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
SQL context available as sqlContext.

scala> import com.mongodb.spark._
import com.mongodb.spark._

scala> import com.mongodb.spark.rdd.MongoRDD
import com.mongodb.spark.rdd.MongoRDD

scala> import org.bson.Document
import org.bson.Document

scala> val rdd = sc.loadFromMongoDB()
rdd: com.mongodb.spark.rdd.MongoRDD[org.bson.Document] = MongoRDD[0] at RDD at MongoRDD.scala:160

scala> println( rdd.count() )
375

scala> println( rdd.first() )
Document{{_id=58d92438ad40331e9f665b20, EVA #=1, Country=USA, Crew=Ed White, Vehicle=Gemini IV, Date=06/03/1965, Duration=0:36, Purpose=First U.S. EVA. Used HHMU and took  photos.
 Gas flow cooling of 25ft umbilical overwhelmed by vehicle ingress work and helmet fogged.  Lost overglove.  Jettisoned thermal gloves and helmet sun visor}}

scala> for( doc <- rdd.take ( 10 ) ) println( doc )
Document{{_id=58d92438ad40331e9f665b20, EVA #=1, Country=USA, Crew=Ed White, Vehicle=Gemini IV, Date=06/03/1965, Duration=0:36, Purpose=First U.S. EVA. Used HHMU and took  photos.
 Gas flow cooling of 25ft umbilical overwhelmed by vehicle ingress work and helmet fogged.  Lost overglove.  Jettisoned thermal gloves and helmet sun visor}}
Document{{_id=58d92438ad40331e9f665b21, EVA #=3, Country=USA, Crew=Eugene Cernan, Vehicle=Gemini IX-A, Date=06/05/1966, Duration=2:07, Purpose=Inadequate restraints, stiff 25ft umb
ilical and high workloads exceeded suit vent loop cooling capacity and caused fogging.  Demo called off of tethered astronaut maneuvering unit}}
Document{{_id=58d92438ad40331e9f665b22, EVA #=6, Country=USA, Crew=Richard Gordon, Vehicle=Gemini XI, Date=09/13/1966, Duration=0:44, Purpose=Attached tether between Agena and Gemi
ni.  EVA ended early due to fatigue, overheating & eye sweat}}
Document{{_id=58d92438ad40331e9f665b23, EVA #=7, Country=USA, Crew=Richard Gordon, Vehicle=Gemini XI, Date=09/14/1966, Duration=2:10, Purpose=Standup EVA.  Took star photos. Agena
tether ops}}
Document{{_id=58d92438ad40331e9f665b24, EVA #=8, Country=USA, Crew=Buzz Aldrin, Vehicle=Gemini XII, Date=11/12/1966, Duration=2:29, Purpose=Standup EVA.  Science tasks.  Took star
photos}}
Document{{_id=58d92438ad40331e9f665b25, EVA #=9, Country=USA, Crew=Buzz Aldrin, Vehicle=Gemini XII, Date=11/13/1966, Duration=2:06, Purpose=Attached tether between Agena and Gemini
.  UV photos of stars.  Waist tether and Dutch shoe eval}}
Document{{_id=58d92438ad40331e9f665b26, EVA #=10, Country=USA, Crew=Buzz Aldrin, Vehicle=Gemini XII, Date=11/14/1966, Duration=0:55, Purpose=Standup EVA.  Jettisoned equipment.  To
ok photos}}
Document{{_id=58d92438ad40331e9f665b27, EVA #=2, Country=USA, Crew=David Scott, Vehicle=Gemini VIII, Date=March 16-17, 1966, Duration=0:00, Purpose=HHMU EVA cancelled before starti
ng by stuck on vehicle thruster that ended mission early}}
Document{{_id=58d92438ad40331e9f665b28, EVA #=12, Country=USA, Crew=Russ Schweickart, Vehicle=Apollo 9, Date=03/06/1969, Duration=0:51, Purpose=Lunar module based.  Took photos.  E
valuated foot restraint and handrails.  Retrieved thermal experiment samples.  First use of PLSS followed by recharge demo after EVA}}
Document{{_id=58d92438ad40331e9f665b29, EVA #=5, Country=USA, Crew=Mike Collins, Vehicle=Gemini X, Date=07/20/1966, Duration=0:39, Purpose=Retrieved MMOD experiment from docked Age
na.  Used HHMU. Lost camera and retrieved experiment. EVA ended early by unrelated spacecraft problem}}

scala> import com.mongodb.spark.config._
import com.mongodb.spark.config._

scala> val readConf = ReadConfig( sc )
readConf: com.mongodb.spark.config.ReadConfig.Self = ReadConfig(nasa,eva,Some(mongodb://127.0.0.1/nasa.eva),1000,64,_id,15,ReadPreferenceConfig(primary,None),ReadConcernConfig(None
))

scala> val readConfig = ReadConfig( Map("collection" -> "eva" ), Some(readConf))
readConfig: com.mongodb.spark.config.ReadConfig = ReadConfig(nasa,eva,Some(mongodb://127.0.0.1/nasa.eva),1000,64,_id,15,ReadPreferenceConfig(primary,None),ReadConcernConfig(None))

scala> val newRDD = sc.loadFromMongoDB( readConfig = readConfig )
newRDD: com.mongodb.spark.rdd.MongoRDD[org.bson.Document] = MongoRDD[1] at RDD at MongoRDD.scala:160

scala> println( rdd.first() )
Document{{_id=58d92438ad40331e9f665b20, EVA #=1, Country=USA, Crew=Ed White, Vehicle=Gemini IV, Date=06/03/1965, Duration=0:36, Purpose=First U.S. EVA. Used HHMU and took  photos.
 Gas flow cooling of 25ft umbilical overwhelmed by vehicle ingress work and helmet fogged.  Lost overglove.  Jettisoned thermal gloves and helmet sun visor}}

scala> def  breakoutCrew (  document: Document  ): List[(String,Int)]  = {
     |
     |   var minutes = 0;
     |   val timeString = document.get( "Duration").asInstanceOf[String]
     |   if( timeString != null && !timeString.isEmpty ) {
     |     val time =  document.get( "Duration").asInstanceOf[String].split( ":" )
     |     minutes = time(0).toInt * 60 + time(1).toInt
     |   }
     |
     |   import scala.util.matching.Regex
     |   val pattern = new Regex("(\\w+\\s\\w+)")
     |   val names =  pattern findAllIn document.get( "Crew" ).asInstanceOf[String]
     |   var tuples : List[(String,Int)] = List()
     |   for ( name <- names ) { tuples = tuples :+ (( name, minutes ) ) }
     |
     |   return tuples
     | }
breakoutCrew: (document: org.bson.Document)List[(String, Int)]

scala> val logs = rdd.flatMap( breakoutCrew ).reduceByKey( (m1: Int, m2: Int) => ( m1 + m2 ) )
logs: org.apache.spark.rdd.RDD[(String, Int)] = ShuffledRDD[3] at reduceByKey at <console>:39

scala> logs.foreach( println )
(Ken Bowersox,797)
(Don Peterson,257)
(Vladimir Dzhanibekov,512)
(Mike Collins,89)
(Claude Nicollier,490)
(Alexandr Ivanchenkov,125)
(Woody Spring,746)
(Ron Garan,1623)
(Vladimir Lyakhov,427)
(Patrick Forrester,705)
(Harrison Schmidt,1333)
(Gennady Strekalov,1322)
(Yuri Lonchakov,627)
(Bill Pogue,814)
(Valentin Lebedev,153)
(Hans Schlegel,405)
(Gennady Padalka,1601)
(Louc Chretien,357)
(Mikhail Kornienko,402)
(Dave Leestma,209)
(Heide Stefanyshyn,2022)
(Stan Love,923)
(Mike Massimino,1844)
(Joe Kerwin,245)
(Oleg Skripochka,1001)
(Victor Savinykh,298)
(Dave Wolf,2516)
(Oleg Kotov,1006)
(Joe Allen,710)
(Rich Clifford,363)
(Greg Chamitoff,823)
(Musa Manarov,2058)
(Nicole Stott,395)
(Anatoli Solovyov,1753)
(Nick Patrick,1093)
(Anton Shkaplerov,375)
(Alexandr Serebrov,1913)
(Alan Shepard,567)
(Dan Burbank,431)
(Alexandr Balandin,652)
(Jim Voss,1365)
(Jeff Wisoff,1193)
(Jack Lousma,662)
(Sergei Zaletin,330)
(Valeri Tsibliev,297)
(Mike Fincke,2916)
(Sunita Williams,3040)
(John Grunsfeld,3510)
(Alexandr Poleshchuk,597)
(James Irwin,1120)
(Pavel Vinogradov,2419)
(Richard Gordon,174)
(Thomas Reiter,856)
(Danny Olivas,2068)
(Linda Godwin,615)
(Anatoly Solovyev,3086)
(Oleg Kononenko,1108)
(Tammy Jernigan,475)
(Andrew Feustel,2538)
(Mike Good,958)
(Mike Gernhardt,1396)
(Alvin Drew,768)
(Leroy Chiao,2176)
(Alexei Leonov,12)
(Rick Mastracchio,2311)
(Alexandr Viktorenko,1184)
(Dave Williams,1067)
(Chris Cassidy,1875)
(Kathy Thornton,1270)
(Yuri Onufrenko,1830)
(Robert Satcher,739)
(Richard Arnold,754)
(Alexander Kaleri,1318)
(Leonid Kizim,1889)
(Christer Fugelsang,820)
(Alexander Samokutyaev,383)
(Doug Wheelock,1241)
(Aleksei Yeliseyov,37)
(Vladimir Solovyov,1889)
(Russ Schweickart,51)
(Yuri Usachev,1849)
(Ed White,36)
(Roman Romanenko,398)
(Bill McArthur,1462)
(Alexandr Volkov,610)
(Steve Smith,2988)
(Christer Fuglesang,1094)
(Jim Newman,2593)
(Story Musgrave,1579)
(Frank Culbertson,304)
(Dmitry Kondratiev,614)
(Bob Behnken,1093)
(Tom Akers,1270)
(Alexandr Kaleri,123)
(Neil Armstrong,157)
(Vladimir Kovalyonok,125)
(Robert Curbeam,1545)
(Don Pettit,797)
(Allen Bean,613)
(Mikhail Tyurin,1532)
(Sergei Avdeyev,2518)
(Tim Kopra,332)
(Victor Afanasyev,2314)
(Lee Morin,847)
(Rick Hieb,553)
(Valery Korzun,1339)
(Yuri Romanenko,615)
(Tom Marshburn,1469)
(Bruce McCandless,725)
(Mario Runco,268)
(Victor Lazerov,0)
(Bill Lenoir,0)
(Greg Harbaugh,1109)
(Eugene Cernan,1460)
(Peggy Whitson,2386)
(Valeri Tsibliyev,852)
(Dan Barry,1549)
(Yuri Onufrienko,359)
(John Herrington,1195)
(Anatoli Berezovsky,153)
(Bob Curbeam,1189)
(Bernard Harris,279)
(Pierre Heignere,379)
(Jerry Ross,3501)
(Alexandr Laveykin,527)
(Gennady Manakov,782)
(Yri Onufrienko,363)
(Clay Anderson,1091)
(Alexandr Alexandrov,344)
(Ed Mitchell,567)
(Jay Apt,626)
(Ed Gibson,921)
(Alexander Misurkin,1201)
(Mike Lopez,3344)
(Dave Griggs,190)
(Yuri Malenchenko,1436)
(Shane Kimbrough,772)
(Anatoli Artsebarsky,1937)
(Paul Weitz,136)
(Andy Thomas,381)
(Mike Foale,1364)
(Jim Reilly,1843)
(Philippe Perrin,1170)
(John Phillips,298)
(Steve Bowen,2838)
(Mark Lee,1561)
(Kathy Sullivan,209)
(Doug Wheellock,1369)
(Susan Helms,536)
(Buzz Aldrin,487)
(Sergei Treschev,321)
(Nikola Budarin,2672)
(Robert Behnken,1159)
(Jerry Linenger,297)
(David Scott,1200)
(Mike Fossum,2912)
(Jeff Williams,1149)
(Winston Scott,1175)
(Talgat Musabeyev,2478)
(Vladimir Dezhurov,2257)
(Sergei Krikalev,2488)
(Svetlana Savitskaya,214)
(Michael Good,835)
(Randy Bresnick,710)
(Joseph Acaba,777)
(Yevgeni Khrunov,37)
(John Young,1218)
(Tom Jones,1189)
(Dan Bursch,708)
(Bill Fisher,698)
(Dale Gardner,710)
(Carl Walz,1137)
(Steve MacLean,431)
(Takao Doi,762)
(Valeri Tokarev,666)
(Paul Richards,381)
(Owen Garriott,823)
(Valeri Ryumin,83)
(Salizhan Sharipov,598)
(Al Worden,39)
(Mike Foreman,1939)
(Clayton Anderson,1218)
(Oleg Makarov,0)
(Akihiko Hoshide,1283)
(Mike Barratt,306)
(Scott Parazynski,2825)
(Fyodor Yurchikhin,2324)
(Carl Meade,411)
(Dan Tani,2351)
(James Van,1296)
(Pete Conrad,753)
(Carlos Noriega,1160)
(Rex Walheim,2183)
(George Nelson,598)
(Maxim Suraev,344)
(Georgi Grechko,88)
(Charles Duke,1218)
(Jeff Hoffman,1512)
(Chris Hadfield,890)
(Steve Robinson,1205)
(Yuri Gidzenko,218)
(Tom Mattingly,83)
(Yuri Malenchecko,374)
(Michael Lopez,716)
(Gerald Carr,949)
(Pierre Thuot,553)
(Ron Evans,67)
(Edward Lu,374)
(Bob Stewart,725)
(Joe Tanner,2789)
(Pat Forrester,825)
(Piers Sellers,2470)
(Soichi Noguchi,1205)
(Vladimir Titov,1113)
(Tracy Caldwell,1369)
(David Low,350)
(Steve Swanson,1582)
(Fyodor Yurchikin,789)
(Sergei Volkov,1116)
(Garrett Reisman,1272)
(Luca Parmitano,459)
(Rick Linnehan,2531)

scala> val writeConf = WriteConfig(sc)
writeConf: com.mongodb.spark.config.WriteConfig.Self = WriteConfig(nasa,astronautTotals,15,Some(mongodb://127.0.0.1/nasa.astronautTotals),WriteConcernConfig(None,None,None,None))

scala> val writeConfig = WriteConfig(Map("collection" -> "astronautTotals", "writeConcern.w" -> "majority", "db" -> "nasa"), Some(writeConf))
writeConfig: com.mongodb.spark.config.WriteConfig = WriteConfig(nasa,astronautTotals,15,Some(mongodb://127.0.0.1/nasa.astronautTotals),WriteConcernConfig(None,Some(majority),None,N
one))

scala> def mapToDocument( tuple: (String, Int )  ): Document = {
     |   val doc = new Document();
     |   doc.put( "name", tuple._1 )
     |   doc.put( "minutes", tuple._2 )
     |
     |   return doc
     | }
mapToDocument: (tuple: (String, Int))org.bson.Document

scala> logs.map( mapToDocument ).saveToMongoDB( writeConfig )

scala> val rdd = sc.loadFromMongoDB()
rdd: com.mongodb.spark.rdd.MongoRDD[org.bson.Document] = MongoRDD[5] at RDD at MongoRDD.scala:160

scala> val aggRdd = rdd.withPipeline(Seq(Document.parse("{ $match: { Country: \"USA\" } }")))
aggRdd: com.mongodb.spark.rdd.MongoRDD[org.bson.Document] = MongoRDD[6] at RDD at MongoRDD.scala:160

scala> import org.apache.spark.sql.SQLContext
import org.apache.spark.sql.SQLContext

scala> import com.mongodb.spark.sql._
import com.mongodb.spark.sql._

scala> val evadf = sqlContext.read.mongo()
evadf: org.apache.spark.sql.DataFrame = [Country: string, Crew: string, Date: string, Duration: string, EVA #: conflict, Purpose: string, Vehicle: string, _id: string]

scala> evadf.registerTempTable("evas")

scala> evadf.show()
+-------+--------------------+-----------------+--------+-----+--------------------+-----------+--------------------+
|Country|                Crew|             Date|Duration|EVA #|             Purpose|    Vehicle|                 _id|
+-------+--------------------+-----------------+--------+-----+--------------------+-----------+--------------------+
|    USA|            Ed White|       06/03/1965|    0:36|    1|First U.S. EVA. U...|  Gemini IV|58d92438ad40331e9...|
|    USA|       Eugene Cernan|       06/05/1966|    2:07|    3|Inadequate restra...|Gemini IX-A|58d92438ad40331e9...|
|    USA|      Richard Gordon|       09/13/1966|    0:44|    6|Attached tether b...|  Gemini XI|58d92438ad40331e9...|
|    USA|      Richard Gordon|       09/14/1966|    2:10|    7|Standup EVA.  Too...|  Gemini XI|58d92438ad40331e9...|
|    USA|         Buzz Aldrin|       11/12/1966|    2:29|    8|Standup EVA.  Sci...| Gemini XII|58d92438ad40331e9...|
|    USA|         Buzz Aldrin|       11/13/1966|    2:06|    9|Attached tether b...| Gemini XII|58d92438ad40331e9...|
|    USA|         Buzz Aldrin|       11/14/1966|    0:55|   10|Standup EVA.  Jet...| Gemini XII|58d92438ad40331e9...|
|    USA|         David Scott|March 16-17, 1966|    0:00|    2|HHMU EVA cancelle...|Gemini VIII|58d92438ad40331e9...|
|    USA|    Russ Schweickart|       03/06/1969|    0:51|   12|Lunar module base...|   Apollo 9|58d92438ad40331e9...|
|    USA|        Mike Collins|       07/20/1966|    0:39|    5|Retrieved MMOD ex...|   Gemini X|58d92438ad40331e9...|
|    USA|        Mike Collins|       07/19/1966|    0:50|    4|Standup EVA.  UV ...|   Gemini X|58d92438ad40331e9...|
|    USA|Neil Armstrong   ...|       07/20/1969|    2:32|   13|First to walk on ...|  Apollo 11|58d92438ad40331e9...|
|    USA|         David Scott|       03/06/1969|    0:47|   11|Standup EVA from ...|   Apollo 9|58d92438ad40331e9...|
|    USA|Allen Bean       ...|       11/20/1969|    0:05|   17|Jettison suit bac...|  Apollo 12|58d92438ad40331e9...|
|    USA|Allen Bean       ...|       11/19/1969|    3:39|   15|Collected 75.6 lb...|  Apollo 12|58d92438ad40331e9...|
|    USA|Ed Mitchell      ...|       02/06/1971|    4:34|   19|Sought but did no...|  Apollo 14|58d92438ad40331e9...|
|    USA|Neil Armstrong   ...|       07/20/1969|    0:05|   14|Jettison suit bac...|  Apollo 11|58d92438ad40331e9...|
|    USA|         David Scott|       07/30/1971|    0:33|   21|Standup EVA to sc...|  Apollo 15|58d92438ad40331e9...|
|    USA|Allen Bean       ...|       11/20/1969|    3:48|   16|Retrieved parts o...|  Apollo 12|58d92438ad40331e9...|
|    USA|Ed Mitchell      ...|       02/06/1971|    0:05|   20|Jettison suit bac...|  Apollo 14|58d92438ad40331e9...|
+-------+--------------------+-----------------+--------+-----+--------------------+-----------+--------------------+
only showing top 20 rows


scala> evadf.count()
res8: Long = 375

scala> evadf.printSchema()
root
 |-- Country: string (nullable = true)
 |-- Crew: string (nullable = true)
 |-- Date: string (nullable = true)
 |-- Duration: string (nullable = true)
 |-- EVA #: conflict (nullable = true)
 |-- Purpose: string (nullable = true)
 |-- Vehicle: string (nullable = true)
 |-- _id: string (nullable = true)


scala> case class astronautTotal ( name: String, minutes: Integer )
defined class astronautTotal

scala> val astronautDF = sqlContext.read.option("collection", "astronautTotals").mongo[astronautTotal]()
astronautDF: org.apache.spark.sql.DataFrame = [name: string, minutes: int]

scala> astronautDF.registerTempTable("astronautTotals")

scala> astronautDF.printSchema()
root
 |-- name: string (nullable = true)
 |-- minutes: integer (nullable = true)


scala> sqlContext.sql("SELECT astronautTotals.name, astronautTotals.minutes FROM astronautTotals"  ).show()
+--------------------+-------+
|                name|minutes|
+--------------------+-------+
|        Ken Bowersox|    797|
|        Don Peterson|    257|
|Vladimir Dzhanibekov|    512|
|        Mike Collins|     89|
|    Claude Nicollier|    490|
|Alexandr Ivanchenkov|    125|
|        Woody Spring|    746|
|           Ron Garan|   1623|
|    Vladimir Lyakhov|    427|
|   Patrick Forrester|    705|
|    Harrison Schmidt|   1333|
|   Gennady Strekalov|   1322|
|      Yuri Lonchakov|    627|
|          Bill Pogue|    814|
|    Valentin Lebedev|    153|
|       Hans Schlegel|    405|
|     Gennady Padalka|   1601|
|       Louc Chretien|    357|
|   Mikhail Kornienko|    402|
|        Dave Leestma|    209|
+--------------------+-------+
only showing top 20 rows


scala> sqlContext.sql("SELECT astronautTotals.name, astronautTotals.minutes, evas.Vehicle, evas.Duration FROM " +
     |   "astronautTotals JOIN evas ON astronautTotals.name LIKE evas.Crew"  ).show()
+----------------+-------+-----------+--------+
|            name|minutes|    Vehicle|Duration|
+----------------+-------+-----------+--------+
|    Mike Collins|     89|   Gemini X|    0:39|
|    Mike Collins|     89|   Gemini X|    0:50|
|  Richard Gordon|    174|  Gemini XI|    0:44|
|  Richard Gordon|    174|  Gemini XI|    2:10|
|   Alexei Leonov|     12|  Voskhod 2|    0:12|
|Russ Schweickart|     51|   Apollo 9|    0:51|
|        Ed White|     36|  Gemini IV|    0:36|
|   Eugene Cernan|   1460|Gemini IX-A|    2:07|
|     Buzz Aldrin|    487| Gemini XII|    2:29|
|     Buzz Aldrin|    487| Gemini XII|    2:06|
|     Buzz Aldrin|    487| Gemini XII|    0:55|
|     David Scott|   1200|Gemini VIII|    0:00|
|     David Scott|   1200|   Apollo 9|    0:47|
|     David Scott|   1200|  Apollo 15|    0:33|
|       Al Worden|     39|  Apollo 15|    0:39|
|   Tom Mattingly|     83|  Apollo 16|    1:23|
|       Ron Evans|     67|  Apollo 17|    1:07|
+----------------+-------+-----------+--------+


scala> :h
h is ambiguous: did you mean :help or :history or :h??

scala> :help
All commands can be abbreviated, e.g. :he instead of :help.
Those marked with a * have more detailed help, e.g. :help imports.

:cp <path>                 add a jar or directory to the classpath
:help [command]            print this summary or command-specific help
:history [num]             show the history (optional num is commands to show)
:h? <string>               search the history
:imports [name name ...]   show import history, identifying sources of names
:implicits [-v]            show the implicits in scope
:javap <path|class>        disassemble a file or class name
:load <path>               load and interpret a Scala file
:paste                     enter paste mode: all input up to ctrl-D compiled together
:quit                      exit the repl
:replay                    reset execution and replay all previous commands
:reset                     reset the repl to its initial state, forgetting all session entries
:sh <command line>         run a shell command (result is implicitly => List[String])
:silent                    disable/enable automatic printing of results
:fallback
disable/enable advanced repl changes, these fix some issues but may introduce others.
This mode will be removed once these fixes stablize
:type [-v] <expr>          display the type of an expression without evaluating it
:warnings                  show the suppressed warnings from the most recent line which had any

scala> :q
Stopping spark context.
17/03/27 20:52:02 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\Aspire\AppData\Local\Temp\spark-6d9fe03e-0123-4ca7-978f-81f474d1d2ef\userFiles-b0159568-0c19-4f54
-b765-8c0928c0b8a9
java.io.IOException: Failed to delete: C:\Users\Aspire\AppData\Local\Temp\spark-6d9fe03e-0123-4ca7-978f-81f474d1d2ef\userFiles-b0159568-0c19-4f54-b765-8c0928c0b8a9
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:928)
        at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
        at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
        at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1229)
        at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:40)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:45)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:47)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:49)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:51)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:53)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:55)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:57)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:59)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC$$iwC.<init>(<console>:61)
        at $line70.$read$$iwC$$iwC$$iwC$$iwC.<init>(<console>:63)
        at $line70.$read$$iwC$$iwC$$iwC.<init>(<console>:65)
        at $line70.$read$$iwC$$iwC.<init>(<console>:67)
        at $line70.$read$$iwC.<init>(<console>:69)
        at $line70.$read.<init>(<console>:71)
        at $line70.$read$.<init>(<console>:75)
        at $line70.$read$.<clinit>(<console>)
        at $line70.$eval$.<init>(<console>:7)
        at $line70.$eval$.<clinit>(<console>)
        at $line70.$eval.$print(<console>)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.repl.SparkIMain$ReadEvalPrint.call(SparkIMain.scala:1065)
        at org.apache.spark.repl.SparkIMain$Request.loadAndRun(SparkIMain.scala:1346)
        at org.apache.spark.repl.SparkIMain.loadAndRunReq$1(SparkIMain.scala:840)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:871)
        at org.apache.spark.repl.SparkIMain.interpret(SparkIMain.scala:819)
        at org.apache.spark.repl.SparkILoop.reallyInterpret$1(SparkILoop.scala:857)
        at org.apache.spark.repl.SparkILoop.interpretStartingWith(SparkILoop.scala:902)
        at org.apache.spark.repl.SparkILoop.command(SparkILoop.scala:814)
        at org.apache.spark.repl.SparkILoop$$anonfun$sparkCleanUp$1.apply(SparkILoop.scala:175)
        at org.apache.spark.repl.SparkILoop$$anonfun$sparkCleanUp$1.apply(SparkILoop.scala:175)
        at org.apache.spark.repl.SparkIMain.beQuietDuring(SparkIMain.scala:324)
        at org.apache.spark.repl.SparkILoop.sparkCleanUp(SparkILoop.scala:174)
        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$closeInterpreter(SparkILoop.scala:181)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply$mcZ$sp(SparkILoop.scala:999)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
        at org.apache.spark.repl.SparkILoop$$anonfun$org$apache$spark$repl$SparkILoop$$process$1.apply(SparkILoop.scala:945)
        at scala.tools.nsc.util.ScalaClassLoader$.savingContextLoader(ScalaClassLoader.scala:135)
        at org.apache.spark.repl.SparkILoop.org$apache$spark$repl$SparkILoop$$process(SparkILoop.scala:945)
        at org.apache.spark.repl.SparkILoop.process(SparkILoop.scala:1059)
        at org.apache.spark.repl.Main$.main(Main.scala:31)
        at org.apache.spark.repl.Main.main(Main.scala)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:498)
        at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:731)
        at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:181)
        at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:206)
        at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:121)
        at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
17/03/27 20:52:03 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Aspire\AppData\Local\Temp\spark-c386b7e1-0803-426f-ab1d-735cd58f59f6
java.io.IOException: Failed to delete: C:\Users\Aspire\AppData\Local\Temp\spark-c386b7e1-0803-426f-ab1d-735cd58f59f6
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:928)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/27 20:52:03 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Aspire\AppData\Local\Temp\spark-6d9fe03e-0123-4ca7-978f-81f474d1d2ef\userFiles-b01595
68-0c19-4f54-b765-8c0928c0b8a9
java.io.IOException: Failed to delete: C:\Users\Aspire\AppData\Local\Temp\spark-6d9fe03e-0123-4ca7-978f-81f474d1d2ef\userFiles-b0159568-0c19-4f54-b765-8c0928c0b8a9
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:928)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/03/27 20:52:03 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\Aspire\AppData\Local\Temp\spark-6d9fe03e-0123-4ca7-978f-81f474d1d2ef
java.io.IOException: Failed to delete: C:\Users\Aspire\AppData\Local\Temp\spark-6d9fe03e-0123-4ca7-978f-81f474d1d2ef
        at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:928)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
        at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
        at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
        at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1765)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
        at scala.util.Try$.apply(Try.scala:161)
        at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
        at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
        at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)

E:\Working\MongoDB\Spark\spark-1.6.1\bin>